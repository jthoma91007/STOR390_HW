---
title: "HW 3"
author: "Jacob Thoma"
date: "11/27/2023"
output: 
  html_document:
    number_sections: true
---

# 

In this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(tidyverse)
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)
```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
a = sample(c(1:200), 100, replace=FALSE)
train = dat[a,]


svmfit = svm(y~.,data=train, kernel="radial", cost=1, scale=FALSE, gamma=1)
print(svmfit)

#plotting this can be easily done with plot(model, data)
plot(svmfit, train)
```

##

*Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. *

```{r}
svmfit = svm(y~.,data=train, kernel="radial", cost=10000, scale=FALSE, gamma=1)
print(svmfit)

#plotting this can be easily done with plot(model, data)
plot(svmfit, train)
```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

There are at least two considerations associated with a high cost model such as the one above. 

*1) A model with such a high cost prioritizes classification of training points over those of testing points. The random distribution of x coordinates means that the training data and testing data are not likely to have similar distributions of points. This is important to consider when factoring in the cost of the model during training, as the current cost of 10000 appears to cause over-fitting to the training data. Because of the janky corners and edges that come with a high cost model like above, the accuracy of the model is likely to suffer when classifying the testing data.*

*2) A model with such a high cost yeilds a graphic that lacks interpretability.It is far easier to quantify a roughly circular boundary (when cost = 1) than it is to quantify the harsh edges of the cost = 10000 graph. There is no explanation for the unique shape of my decision boundary besides overfitting. As such, the overfitting of this model makes the decision boundary challenging to translate to other data.*


##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r}
#remove eval = FALSE in above
table(true=dat[-a,"y"], pred=predict(svmfit, newdata=dat[-a,]))

18/(59+18)
5/(5+18)
```
*With class imbalance taken into consideration, there is really not that much disparity in the classification results. The model seems to classify classes 1 and 2 with reasonably similar levels of accuracy. Based on the phrasing of the next question I feel like my answer is wrong.*

##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}
train_prop = nrow(filter(train, y==2))/nrow(train)
dat_prop = nrow(filter(dat, y==2))/nrow(dat)

train_prop
dat_prop

```
*It seems that the randomly generated training partition has a similar proportion of class 2 observations to the overall data.*

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  


```{r}
set.seed(1)

tuning_parameters <- expand.grid(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4))

tune.out <- tune(svm, y ~ ., data = train, kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)))

summary(tune.out)
```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  
```{r}
table(true=dat[-a,"y"], pred=predict(tune.out$best.model, newdata=dat[-a,]))
10/77
2/23
```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

*The error rates in the svm classificaiton have improved significantly from the original model. There is room for improvement in this classification model. While both of the error rates have improved significantly, there seems to be a larger difference between misclassificaiton proporitions of class 1 and class 2. This is something that could be looked at with more data or different cross validation methods to determine if it is a problem.*

# 
Let's turn now to decision trees.  

```{r}
library(kmed)
data(heart)
library(tree)
```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}
heart = heart %>% mutate(heart_disease = ifelse(class == 0, 0, 1))
heart$heart_disease = as.factor(heart$heart_disease)
heart = select(heart, -c(14))
```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)
train=sample(1:nrow(heart), 240)

heart.disease = tree(heart_disease~., heart, subset = train)
plot(heart.disease)
text(heart.disease, pretty = 0)
```

## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}
heart.pred = predict(heart.disease, heart[-train,], type="class")
with(heart[-train,], table(heart.pred, heart_disease))

misclass = (3+8)/(28+18+3+8)
misclass

```

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}
set.seed(1)
cv.heart <- cv.tree(heart.disease, FUN = prune.misclass)

plot(cv.heart$size, cv.heart$dev, type = "b")
```
*Based on the above graph, the optimal number of splits for pruning is 4.*

```{r}
prune.heart = prune.misclass(heart.disease, best=4)

plot(prune.heart)
text(prune.heart, pretty=0)

heart.pred = predict(prune.heart, heart[-train,], type="class")
with(heart[-train,], table(heart.pred, heart_disease))

misclass = (10+4)/(26+4+10+17)
misclass
```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

*While the pruned decision tree has a higher mis-classification rate by 5%, it is a far more interpretable model than the non-pruned tree. The overall implications of a more interpretable model with moderately less accuracy are net positive in this case. In this use case, the pruned model means that doctors can be almost as accurate in diagnosis by knowing/measuring three different variables opposed to the thirteen variables they measured in heart. The insightfulness of the pruned model could potentially streamline heart diagnoses for the doctors in the future by only measuring a few variables each visit.*

*The other trade off worth considering is in the specific nature of misclassification. The pruned model overall is slightly less accurate in its classifcation based on the empirical results of the two confusion matrices above. It could be argued that these differences are negligible, or that we need more data. For the sake of answering the question, I will assume that these differences are not negligible. The false positive rate increased more than the false negative rate by one classification, and true negative rate decreased more than true positive rate. The consequences of this are less severe than the inverse scenario. It is not as consequential to have more false positives because its better for there not to be a heart problem then for it to go unnoticed. *

## 

Discuss the ways a decision tree could manifest algorithmic bias.  

*A decision tree could manifest algorithmic bias in a few different ways. Underlying bias in the dataset/data collection process could influence which factors are most influential in classifying a target variable, perpetuating algorithmic bias in further splits of the tree based on that initial split. This algorithmic bias reduces the model's translatability to the entire population where the underlying biases may not be true. The underlying bias in the data that could cause algorithmic bias in a decision include bias in the dataset (non-representative of the population), poor construction of the testing/training split, or not having potentially important features in the dataset. *