---
title: "HW 2 Jacob Thoma"
author: "Andy Ackerman"
date: "10/17/2023"
output: 
  html_document:
    number_sections: true
---

This homework is meant to illustrate the methods of classification algorithms as well as their potential pitfalls.  In class, we demonstrated K-Nearest-Neighbors using the `iris` dataset.  Today I will give you a different subset of this same data, and you will train a KNN classifier.  

```{r, echo=FALSE, trace=FALSE}
set.seed(123)
library(class)

df <- data(iris) 

normal <-function(x) {
  (x -min(x))/(max(x)-min(x))   
}

iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], normal))

subset <- c(1:45, 58, 60:70, 82, 94, 110:150)
iris_train <- iris_norm[subset,] 
iris_test <- iris_norm[-subset,] 

iris_target_category <- iris[subset,5]
iris_test_category <- iris[-subset,5]

summary(iris_norm)
```

#
Above, I have given you a training-testing partition.  Train the KNN with $K = 5$ on the training data and use this to classify the 50 test observations.  Once you have classified the test observations, create a contingency table -- like we did in class -- to evaluate which observations your algorithm is misclassifying.   

```{r}
set.seed(123)
pr <- knn(train=iris_train, test=iris_test, cl= iris_target_category, k=5)
tab <- table(pr, iris_test_category)
tab

accuracy = function(x){
  sum(diag(x))/(sum(rowSums(x)))*100}

accuracy(tab)
```

#

Discuss your results.  If you have done this correctly, you should have a classification error rate that is roughly 20% higher than what we observed in class.  Why is this the case? In particular run a summary of the `iris_test_category` as well as `iris_target_category` and discuss how this plays a role in your answer.  

```{r}
summary(iris_test_category)
summary(iris_target_category)

setosa_test_percentage = 5/50
setosa_iris_percentage = 50/150

setosa_test_percentage
setosa_iris_percentage
```
#
For the following test/train split, my KNN classification algorithm had a 78% accuracy rate on the test data. After looking at iris_test_category and iris_target_category, it is clear that the non-random construction of the test and train data has affected the accuracy of the KNN classification. This is obvious in the problems presented below:

Problem 1: Remembering past analysis from class, it was clear that the setosa category is more distinguishable from the other categories (virginica, versicolor) by the variables measured in the iris dataset. Because the setosa category is more distinguishable by the variables in the dataset, the setosa category is easier for KNN to correctly classify. Only 10% of the testing data is setosa in comparison to 33% of the total iris data. This means that the KNN algorithim has less "easy" setosa points to classify which negatively impacts classification accuracy.

Problem 2: The training data is imbalanced between the less distinguishable categories (versicolor and virginica). This is a problem that heavily impacts the KNN classification of the testing data, as versicolors are more likely to be outnumbered in the simple majority by virginicas due to sheer number of traininig data points in a given area that are virginicas, forcing the KNN model to overpredict the number of virginicas when those data points could likely be versicolors or virginicas. This negatively impacts classification accuracy more than it would in a more randomly constructed test/train split of the iris dataset because more representative training data would allow for more versicolor points to appear in the training data, potentially influencing classificaiton of a testing data set with the KNN classifcaition method of simple majority.

Build a github repository to store your homework assignments.  Share the link in this file.  

*https://github.com/jthoma91007/STOR390_HW*
